{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d07380df",
   "metadata": {},
   "source": [
    "# Pong Behavioral Cloning - Google Colab Training\n",
    "\n",
    "This notebook contains everything needed to train the Pong CNN model on Google Colab.\n",
    "\n",
    "## Setup Instructions:\n",
    "1. Upload `pong_dataset.npz` to Google Drive\n",
    "2. Mount your Google Drive\n",
    "3. Update the `DATA_PATH` below to point to your dataset\n",
    "4. Run all cells"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb6eb69",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c493544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (most are pre-installed on Colab)\n",
    "!pip install -q tensorflow numpy scikit-learn matplotlib\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import os\n",
    "import time\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU available: {len(tf.config.list_physical_devices('GPU')) > 0}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a5a94a",
   "metadata": {},
   "source": [
    "## 2. Mount Google Drive (Optional - for storing dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e66887c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive to access your dataset\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# If you uploaded dataset to Google Drive, update this path:\n",
    "# DATA_PATH = '/content/drive/MyDrive/pong_dataset.npz'\n",
    "\n",
    "# Or if you'll upload directly to Colab session:\n",
    "# DATA_PATH = '/content/pong_dataset.npz'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7bdb16e",
   "metadata": {},
   "source": [
    "## 3. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7411232d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== CONFIGURATION =====\n",
    "# Update this path to your dataset location\n",
    "DATA_PATH = '/content/drive/MyDrive/pong_dataset.npz'  # Change this!\n",
    "\n",
    "# Training parameters\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 20\n",
    "LEARNING_RATE = 1e-4\n",
    "NUM_ACTIONS = 6\n",
    "\n",
    "# Smoke test mode (for quick testing)\n",
    "SMOKE_TEST = False  # Set to True to use subset of data\n",
    "SUBSET_SIZE = 1000  # Only used if SMOKE_TEST=True\n",
    "\n",
    "# Checkpoint directory\n",
    "CHECKPOINT_DIR = '/content/checkpoints'\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Data path: {DATA_PATH}\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Epochs: {EPOCHS}\")\n",
    "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"  Smoke test: {SMOKE_TEST}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa32446",
   "metadata": {},
   "source": [
    "## 4. Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e599781",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pong_cnn(input_shape=(84, 84, 1), num_actions=6):\n",
    "    \"\"\"\n",
    "    Create a Convolutional Neural Network for Pong action prediction.\n",
    "    Architecture inspired by DQN/Atari networks.\n",
    "    \n",
    "    Args:\n",
    "        input_shape: Input image shape (height, width, channels)\n",
    "        num_actions: Number of possible actions (default: 6)\n",
    "    \n",
    "    Returns:\n",
    "        Keras Model\n",
    "    \"\"\"\n",
    "    \n",
    "    # Input layer\n",
    "    inputs = keras.Input(shape=input_shape, name='input_image')\n",
    "    \n",
    "    # Convolutional layers\n",
    "    # Conv1: 32 filters, 8x8 kernel, stride 4\n",
    "    x = layers.Conv2D(32, kernel_size=8, strides=4, activation='relu', \n",
    "                      name='conv1')(inputs)\n",
    "    \n",
    "    # Conv2: 64 filters, 4x4 kernel, stride 2\n",
    "    x = layers.Conv2D(64, kernel_size=4, strides=2, activation='relu',\n",
    "                      name='conv2')(x)\n",
    "    \n",
    "    # Conv3: 64 filters, 3x3 kernel, stride 1\n",
    "    x = layers.Conv2D(64, kernel_size=3, strides=1, activation='relu',\n",
    "                      name='conv3')(x)\n",
    "    \n",
    "    # Flatten\n",
    "    x = layers.Flatten(name='flatten')(x)\n",
    "    \n",
    "    # Fully connected layers\n",
    "    x = layers.Dense(512, activation='relu', name='fc1')(x)\n",
    "    \n",
    "    # Output layer (logits, no activation)\n",
    "    outputs = layers.Dense(num_actions, activation=None, name='output')(x)\n",
    "    \n",
    "    # Create model\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs, name='PongCNN')\n",
    "    \n",
    "    return model\n",
    "\n",
    "print(\"✓ Model architecture defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7193d66",
   "metadata": {},
   "source": [
    "## 5. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63073521",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_path, smoke_test=False, subset_size=1000):\n",
    "    \"\"\"\n",
    "    Load the processed Pong dataset.\n",
    "    \n",
    "    Args:\n",
    "        data_path: Path to the .npz file\n",
    "        smoke_test: If True, use only a small subset\n",
    "        subset_size: Number of samples for smoke test\n",
    "    \n",
    "    Returns:\n",
    "        (X_train, y_train, X_test, y_test)\n",
    "    \"\"\"\n",
    "    print(\"\\nLoading datasets...\")\n",
    "    data = np.load(data_path)\n",
    "    \n",
    "    X_train = data['X_train']\n",
    "    y_train = data['y_train']\n",
    "    X_test = data['X_test']\n",
    "    y_test = data['y_test']\n",
    "    \n",
    "    print(f\"Loaded train set: {X_train.shape[0]} samples\")\n",
    "    print(f\"  Image shape: {X_train.shape[1:]}\")\n",
    "    print(f\"  Label shape: {y_train.shape}\")\n",
    "    print(f\"  Unique actions: {np.unique(y_train)}\")\n",
    "    \n",
    "    print(f\"Loaded test set: {X_test.shape[0]} samples\")\n",
    "    \n",
    "    # Smoke test: use subset\n",
    "    if smoke_test:\n",
    "        print(f\"\\nMOKE TEST MODE: Using {subset_size} samples\")\n",
    "        train_indices = np.random.choice(len(X_train), \n",
    "                                        min(subset_size, len(X_train)), \n",
    "                                        replace=False)\n",
    "        test_indices = np.random.choice(len(X_test), \n",
    "                                       min(subset_size // 4, len(X_test)), \n",
    "                                       replace=False)\n",
    "        X_train = X_train[train_indices]\n",
    "        y_train = y_train[train_indices]\n",
    "        X_test = X_test[test_indices]\n",
    "        y_test = y_test[test_indices]\n",
    "        print(f\"  Train subset: {len(X_train)} samples\")\n",
    "        print(f\"  Test subset: {len(X_test)} samples\")\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "# Load the data\n",
    "X_train, y_train, X_test, y_test = load_data(DATA_PATH, SMOKE_TEST, SUBSET_SIZE)\n",
    "\n",
    "print(\"\\nData loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05cd643d",
   "metadata": {},
   "source": [
    "## 6. Create and Compile Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fab4a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for GPU\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    print(f\"Using GPU: {gpus}\")\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "else:\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "# Create checkpoint directory\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "# Create model\n",
    "print(\"\\nCreating model...\")\n",
    "model = create_pong_cnn(input_shape=(84, 84, 1), num_actions=NUM_ACTIONS)\n",
    "\n",
    "# Print model summary\n",
    "model.summary()\n",
    "\n",
    "# Compile model\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"\\nModel compiled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3675ddb8",
   "metadata": {},
   "source": [
    "## 7. Setup Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0420e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup callbacks\n",
    "callbacks = [\n",
    "    # Save checkpoints\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath=os.path.join(CHECKPOINT_DIR, 'checkpoint_epoch_{epoch:02d}.weights.h5'),\n",
    "        save_weights_only=True,\n",
    "        save_freq='epoch',\n",
    "        verbose=1\n",
    "    ),\n",
    "    # Save best model\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath=os.path.join(CHECKPOINT_DIR, 'best_model.weights.h5'),\n",
    "        save_weights_only=True,\n",
    "        save_best_only=True,\n",
    "        monitor='val_accuracy',\n",
    "        mode='max',\n",
    "        verbose=1\n",
    "    ),\n",
    "    # CSV logger\n",
    "    keras.callbacks.CSVLogger(\n",
    "        os.path.join(CHECKPOINT_DIR, 'training_log.csv'),\n",
    "        separator=',',\n",
    "        append=False\n",
    "    ),\n",
    "    # TensorBoard\n",
    "    keras.callbacks.TensorBoard(\n",
    "        log_dir=os.path.join(CHECKPOINT_DIR, 'logs'),\n",
    "        histogram_freq=0,\n",
    "        write_graph=True,\n",
    "        update_freq='epoch'\n",
    "    )\n",
    "]\n",
    "\n",
    "print(\"Callbacks configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74512aa",
   "metadata": {},
   "source": [
    "## 8. Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0079ba16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "print(f\"\\nStarting training for {EPOCHS} epochs...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=(X_test, y_test),\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Training completed!\")\n",
    "print(f\"Total training time: {total_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d738b5fe",
   "metadata": {},
   "source": [
    "## 9. Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b64d6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final evaluation\n",
    "print(\"\\nFinal evaluation on test set:\")\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"  Test Loss: {test_loss:.4f}\")\n",
    "print(f\"  Test Accuracy: {test_acc * 100:.2f}%\")\n",
    "\n",
    "# Save final model\n",
    "final_model_path = os.path.join(CHECKPOINT_DIR, 'final_model.weights.h5')\n",
    "model.save_weights(final_model_path)\n",
    "print(f\"\\nFinal model saved to {final_model_path}\")\n",
    "\n",
    "# Print best results\n",
    "best_val_acc = max(history.history['val_accuracy'])\n",
    "best_epoch = history.history['val_accuracy'].index(best_val_acc) + 1\n",
    "print(f\"\\nBest validation accuracy: {best_val_acc * 100:.2f}% (Epoch {best_epoch})\")\n",
    "\n",
    "print(\"\\nTraining complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ebfeced",
   "metadata": {},
   "source": [
    "## 10. Plot Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18b3e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training history\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Plot accuracy\n",
    "ax1.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "ax1.plot(history.history['val_accuracy'], label='Val Accuracy')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Accuracy')\n",
    "ax1.set_title('Model Accuracy')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# Plot loss\n",
    "ax2.plot(history.history['loss'], label='Train Loss')\n",
    "ax2.plot(history.history['val_loss'], label='Val Loss')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Loss')\n",
    "ax2.set_title('Model Loss')\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(CHECKPOINT_DIR, 'training_history.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Training history plot saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f558bdd3",
   "metadata": {},
   "source": [
    "## 11. Download Trained Model (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391f2361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the best model to your local machine\n",
    "from google.colab import files\n",
    "\n",
    "# Download best model\n",
    "files.download(os.path.join(CHECKPOINT_DIR, 'best_model.weights.h5'))\n",
    "\n",
    "# Download training log\n",
    "files.download(os.path.join(CHECKPOINT_DIR, 'training_log.csv'))\n",
    "\n",
    "# Download training history plot\n",
    "files.download(os.path.join(CHECKPOINT_DIR, 'training_history.png'))\n",
    "\n",
    "print(\"Files ready for download!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096596d1",
   "metadata": {},
   "source": [
    "## 12. Test Model Predictions (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24ebe52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test predictions on a few random samples\n",
    "num_samples = 5\n",
    "random_indices = np.random.choice(len(X_test), num_samples, replace=False)\n",
    "\n",
    "print(\"Sample Predictions:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for idx in random_indices:\n",
    "    sample = X_test[idx:idx+1]\n",
    "    true_action = y_test[idx]\n",
    "    \n",
    "    # Predict\n",
    "    logits = model.predict(sample, verbose=0)\n",
    "    predicted_action = np.argmax(logits, axis=1)[0]\n",
    "    \n",
    "    match = \"✓\" if predicted_action == true_action else \"✗\"\n",
    "    print(f\"{match} True: {true_action}, Predicted: {predicted_action}\")\n",
    "\n",
    "# Calculate overall accuracy on test set\n",
    "test_predictions = model.predict(X_test, verbose=0)\n",
    "test_pred_actions = np.argmax(test_predictions, axis=1)\n",
    "accuracy = np.mean(test_pred_actions == y_test)\n",
    "print(f\"\\nOverall Test Accuracy: {accuracy * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
